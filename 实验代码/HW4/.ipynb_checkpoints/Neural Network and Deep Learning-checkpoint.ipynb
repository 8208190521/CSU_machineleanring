{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks & Deep Learning\n",
    "在这一节中，我们将构建一个三层的神经网络来处理手写数字识别问题，之后我们将运用AdaGrad、RMSprop、Momentum、Nesterov Momentum和Adam优化算法来加速梯度下降的过程，首先我们先来实现一个简单的神经网络。 \n",
    "\n",
    "## 1. 导入所需的Python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据并可视化\n",
    "先介绍一下在这个实验中所用到的数据库MNIST，MNIST数据集是一个手写体数据集，其中每个手写数字是一张28×28的灰度图片，图片的标记为一个0-9表示的数字。  MNIST数据集一共有60000张图片用来作为训练集，10000张图片来作为测试集。  \n",
    "我们知道一张灰度图片一般是二维的，但是神经网络中的全连接层的输入是一个一维的向量。所以我们需要将一张二维的灰度图片“压扁”成一个一维的向量，具体如下图所示：\n",
    "![2d_to_1d](./images/2d_to_1d.jpg)\n",
    "因此每一个样本都是一个784维的向量。 \n",
    "\n",
    "在处理多分类任务时，我们可以使用softmax来进行处理，这里的手写数字识别任务就是一个多分类任务，共包含有10类，分别用数字0-9表示，而在softmax中，每一类可以表示为一个向量，所以我们需要将类对应的符号标记转化成一个向量表示，这就是one-hot向量，比如，在手写数字识别中，数字0和1对应的one-hot向量分别为： \n",
    "$$one-hot(0)=\\begin{bmatrix}1 \\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix},one-hot(1)=\\begin{bmatrix}0 \\\\ 1\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix}$$ \n",
    "在训练集中，我们需要把样本的标记$Y$转化为one-hot向量。  \n",
    "另外值得注意的一点，在之前的实验中数据的第1个维度一般都是数据样本的索引，在这次实验中，为了保持代码与公式尽量形式一致，我们用第2个维度来表示数据的索引。因此，训练集的shape是$(784, 60000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载训练集或测试集\n",
    "path = './MNIST Data'    #数据集文件所在目录\n",
    "X_train, Y_train = load_mnist(path)    #加载训练集样本X_train、Y_train\n",
    "X_test, Y_test = load_mnist(path, kind='t10k')    #加载测试集样本X_test、Y_test\n",
    "# 转换成one_hot向量\n",
    "Y_train = np.eye(10)[Y_train].T\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "print('The shape of X_train is:',X_train.shape)\n",
    "print('The shape of Y_train is:',Y_train.shape)\n",
    "print('The shape of X_test is:',X_test.shape)\n",
    "print('The shape of Y_test is:',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面在训练集中找几个图片看一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(141)\n",
    "ax1.imshow(X_train[:,1].reshape(28, 28), cmap='Greys')\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax2.imshow(X_train[:,2].reshape(28,28), cmap='Greys')\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.imshow(X_train[:,3].reshape(28,28), cmap='Greys')\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.imshow(X_train[:,4].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n",
    "print('one hot 标签：',Y_train[:,1],Y_train[:,2],Y_train[:,3],Y_train[:,4])\n",
    "print('对应的实际标签：',np.argmax(Y_train[:,1]),np.argmax(Y_train[:,2]),np.argmax(Y_train[:,3]),np.argmax(Y_train[:,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 全连接神经网络\n",
    "\n",
    "接下来，我们来搭建一个三层全连接神经网络。网络结构如下图所示：\n",
    "![network_architecture](./images/network_architecture.jpg)\n",
    "\n",
    "对公式所用符号的说明：  \n",
    "一般算神经网络层数不算输入层，所以上图只有3层。用右上角的方括号表示相应的层，所有第1层的权重$W$为$W^{[1]}$，第1层的偏置项$b$为$b^{[1]}$(图中未标出)，第1层的激活值$A$为$A^{[1]}$。  \n",
    "为了方便，我们将$\\frac{\\partial L}{\\partial W^{[1]}}$简写为$dW^{[1]}$,将$\\frac{\\partial L}{\\partial b^{[1]}}$简写为$db^{[1]}$.  \n",
    "用右下标如$b_i$来表示变量的第$i$个分量，例如，$b^{[3]}_4$表示第3层的偏置项的第4个分量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 初始化网络参数\n",
    "\n",
    "我们规定第1层的神经元个数为300，第二层个数为300，最后一层为10.输入向量$X$的维度为784，那么整个网络对应的参数也就可以确定了。  \n",
    "$W^{[1]}$的shape为$(300,784)$，$b^{[1]}$的shape为$(300,1)$  \n",
    "$W^{[2]}$的shape为$(300,300)$，$b^{[2]}$的shape为$(300,1)$  \n",
    "$W^{[3]}$的shape为$(10,300)$，$b^{[3]}$的shape为$(10,1)$  \n",
    "这里使用随机正态分布再乘上比例因子0.01来初始化$W$， 对$b$都初始化为0.  \n",
    "**Hint**: 使用`np.random.randn()`,`np.zeros()`  \n",
    "**任务1**: 初始化网络参数  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    :return: parameters : python dictionary containing your parameters W1, b1, W2 , b2, W3 and b3\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    W1 = 0.01*np.random.randn(300,784)\n",
    "    b1 = np.zeros(300).reshape(300,1)\n",
    "    W2 = 0.01*np.random.randn(300,300)\n",
    "    b2 = np.zeros(300).reshape(300,1)\n",
    "    W3 = 0.01*np.random.randn(10,300)\n",
    "    b3 = np.zeros(10).reshape(10,1)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    parameters = dict({\n",
    "        'W1':W1,\n",
    "        'b1':b1,\n",
    "        'W2':W2,\n",
    "        'b2':b2,\n",
    "        'W3':W3,\n",
    "        'b3':b3\n",
    "    })\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试一下\n",
    "parameters_test = initialize_parameters()\n",
    "print('shape of W1 and b1',parameters_test['W1'].shape, parameters_test['b1'].shape)\n",
    "print('shape of W2 and b2',parameters_test['W2'].shape, parameters_test['b2'].shape)\n",
    "print('shape of W3 and b3',parameters_test['W3'].shape, parameters_test['b3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 前向传播\n",
    "\n",
    "下面回顾下前向传播的公式  \n",
    "$$Z^{[1]}=W^{[1]}X+b^{[1]}$$\n",
    "$$A^{[1]}=g^{[1]}(Z^{[1]})$$\n",
    "$$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$$\n",
    "$$A^{[2]}=g^{[2]}(Z^{[2]})$$\n",
    "$$Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}$$\n",
    "$$A^{[3]}=g^{[3]}(Z^{[3]})$$  \n",
    "其中，$g^{[i]}$表示第$i$层的激活函数，这里在第1层和第2层都使用$tanh$函数，第3层使用$softmax$用来进行多分类。  \n",
    "$softmax$函数的具体公式为\n",
    "$$softmax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1}^{C}{e^{x_j}}}$$ \n",
    "$x_i$表示为向量$x$的第$i$个分量。  \n",
    "$softmax$函数这里直接给出，在实现的时候会减去一个最大值，防止指数运算结果太大，结果都是一样的。  \n",
    "**Hint**:$tanh$函数可用`np.tanh()`  \n",
    "**任务2**：完成神经网络前向传播  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x, axis=0)  # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    :param X: The input of network\n",
    "    :param parameters: python dictionary containing your parameters W1, b1, W2 , b2, W3 and b3\n",
    "    :return: A3, cache \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    Z3 = np.dot(W3,A2)+b3\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    cache = {\n",
    "        'Z1':Z1,\n",
    "        'A1':A1,\n",
    "        'Z2':Z2,\n",
    "        'A2':A2,\n",
    "        'Z3':Z3,\n",
    "        'A3':A3   \n",
    "    }\n",
    "    return A3, cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试一下\n",
    "X_1 = X_train[:,:4]\n",
    "parameters_test = initialize_parameters()\n",
    "A3, cache  = forward_propagation(X_1, parameters_test)\n",
    "print(A3[:,1]) # 应该有10个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 计算Loss  \n",
    "\n",
    "在softmax中，我们一般使用的损失函数为交叉熵函数(这里的$\\hat{y}$就是上面前向传播最后的输出$A^{[3]}$)：  \n",
    "$$L(\\hat{y},y)=-\\sum_{j=1}^{C}{y_jlog\\hat{y_j}}$$  \n",
    "其中，$C$是类别的数量，在本次实验中即为10.  \n",
    "成本函数为：  \n",
    "$$J(W^{[1]},b^{[1]},...)=\\frac{1}{m}\\sum_{i=1}^{m}{L(\\hat{y}^{(i)},y^{(i)})}$$  \n",
    "这里$m$是mini-batch的大小。因为训练集有60000个，我们不能直接把$(784,60000)$大小的输入直接放入神经网络计算。因此，每次就选择一部分来进行前向传播，所以输入的大小是$(784,m)$，本次实验默认$m=128$.也就是输入$X$的shape为$(784,128)$  \n",
    "**Hint**:  \n",
    "(1) 没必要对上面第一个式子每个分量求和，它的向量化表示是$L(\\hat{Y},Y)=-Y\\log \\hat{Y}$  \n",
    "(2) `np.sum(A)`第二个参数axis不写,就是对整个A求和  \n",
    "(3) 最后的Loss是个标量  \n",
    "**任务3**：计算Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算成本函数\n",
    "def compute_cost(A3, Y):\n",
    "    \"\"\"\n",
    "    :param A3: The output of the second activation\n",
    "    :param Y: the labels of examples\n",
    "    :param parameters: python dictionary containing your parameters W1, b1, W2, b2, W3 and b3\n",
    "    :return: cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]  #样本数量\n",
    "\n",
    "    #计算交叉熵损失\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    loss=-Y*np.log(A3)\n",
    "    cost = np.sum(loss)/m\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    cost = np.squeeze(cost)  # makes sure cost is the dimension we expect.\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_1 = Y_train[:, :4]\n",
    "print(compute_cost(A3, Y_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 反向传播\n",
    "\n",
    "反向传播用到的主要是链式求导法则，我们用dX表示损失Loss对于X的导数，这里直接给出最终的求导结果：$$\\begin{align}\n",
    "& dZ^{[3]}=A^{[3]}-Y  \\\\\n",
    "& dW^{[3]}=\\frac{1}{m}dZ^{[3]}A^{[2]T} \\\\\n",
    "& db^{[3]}=\\frac{1}{m}np.sum(dZ^{[3]},axis=1, keepdims=True) \\\\\n",
    "& dA^{[2]}= W^{[3]^T}dZ^{[3]}\\\\\n",
    "& dZ^{[2]}= dA^{[2]} * g^{[2]'}(Z^{[2]}) \\\\\n",
    "& dW^{[2]}=\\frac{1}{m}dZ^{[2]}A^{[1]T} \\\\\n",
    "& db^{[2]}=\\frac{1}{m}np.sum(dZ^{[2]},axis=1, keepdims=True) \\\\\n",
    "\\end{align}$$\n",
    "至于$dW^{[1]}、db^{[1]}$则需要同学们自己求解  \n",
    "**Hint**：  \n",
    "(1)你可能需要用到`np.dot()`、`np.sum()`、`np.multiply()`、`np.power()`  \n",
    "(2)上式中的$*$表示element-wise乘法即`np.multiply()`，直接写在一起是点乘即`np.dot()`  \n",
    "(3)$g^{[1]'}$和$g^{[2]'}$ 都是$tanh$函数，$tanh$的导数为$g^{[2]'}(x) = 1-tanh^2(x)$ 而$tanh(Z^{[2]})=g^{[2]}(Z^{[2]})=A^{[2]}$\n",
    "\n",
    "**任务4**：完成反向传播函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters W1, b1, W2, b2, W3 and b3\n",
    "    :param cache: storing the result computed before\n",
    "    :param X: the input of network\n",
    "    :param Y: the labels of examples\n",
    "    :return: grads : python dictionary containing gradients of your parameters dW1, db1, dW2, db2, dW3 and db3\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    A3 = cache['A3']\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    dZ3 = A3-Y\n",
    "    dW3 = np.dot(dZ3,A2.T)/m\n",
    "    db3 = np.sum(dZ3,axis=1,keepdims=True)/m\n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "\n",
    "    dZ2 = np.multiply(dA2,1-np.power(A2,2))\n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "    dA1 =np.dot(W2.T,dZ2)#debug\n",
    "\n",
    "    dZ1 = np.multiply(dA1,1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,X.T)/m#debug is erro\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    grads = {\n",
    "        'dW3':dW3,\n",
    "        'db3':db3,\n",
    "        'dW2':dW2,\n",
    "        'db2':db2,\n",
    "        'dW1':dW1,\n",
    "        'db1':db1\n",
    "    }\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward_propagation(parameters_test, cache, X_1, Y_1)\n",
    "print('shape of dW3',grads['dW3'].shape)\n",
    "print('db3:',grads['db3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 参数更新  \n",
    "按照下列公式计算：$$\\begin{align} & W=W-\\alpha dW \\\\ & b=b-\\alpha db\\end{align}$$  \n",
    "**任务5**：对网络参数进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters W1, b1, W2, b2, W3 and b3\n",
    "    :param grads: python dictionary containing gradients of your parameters dW1, db1, dW2, db2, dW3 and db3\n",
    "    :param learning_rate: learning rate\n",
    "    :return: parameters : python dictionary containing your updated parameters W1, b1, W2, b2, W3 and b3\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    dW3 = grads['dW3']\n",
    "    db3 = grads['db3']\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    parameters = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2,\n",
    "                  'W3': W3,\n",
    "                  'b3': b3}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    :param X: input data of size (n_x, m)\n",
    "    :param parameters: python dictionary containing your parameters\n",
    "    :return: predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    y_hat, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(y_hat, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 搭积木\n",
    "\n",
    "前面描述损失函数的时候简单提了下mini-batch,我们之前学过的梯度下降法称为batch梯度下降法，在batch梯度下降法的一次迭代过程中，我们必须遍历整个训练数据集然后才能进行一次梯度下降，当数据集比较大的时候，这个过程会比较慢。为了加快梯度下降的速度，我们将数据集分块，每遍历一个数据块，都进行一次梯度下降，遍历完一次整个数据集称为一代(epoch).  \n",
    "在此次实验中，我们将mini-batch的大小设为128。`random_mini_batches`函数返回的mini_batches是一个列表，列表的元素是一小块一小块的X、Y组成的数据块，除了最后一个batch，前面每个batch都包含128个样本。  \n",
    "**任务6**：搭建整个网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义整个模型\n",
    "def nn_model(X, Y, num_iterations=1000, learning_rate=0.001, mini_batch_size = 128, \n",
    "             beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-7, optimization='GD'):\n",
    "    \"\"\"\n",
    "    :param X: input data\n",
    "    :param Y: the labels of X\n",
    "    :param num_iterations: Number of iterations in gradient descent loop\n",
    "    :param learning_rate: learning rate\n",
    "    :param mini_batch_size: size of the mini-batches, integer\n",
    "    :param beta1: parameter of momentum and adam\n",
    "    :param beta2: parameter of rmsprop and adam\n",
    "    :param epsilon: small constant\n",
    "    :param optimization: choose optimization\n",
    "    :return: parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "    cost = 0\n",
    "    seed = 1\n",
    "    t = 0\n",
    "    test_accuracy = []\n",
    "    \n",
    "    # 初始化参数\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    if optimization == 'GD':\n",
    "        pass\n",
    "    elif optimization == 'AdaGrad':\n",
    "        grads_squared = initialize_grads_squared(parameters)\n",
    "    elif optimization == 'RMSprop':\n",
    "        grads_squared = initialize_grads_squared(parameters)\n",
    "    elif optimization == 'Momentum':\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimization == 'Nesterov':\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimization == 'Adam':\n",
    "        v, u = initialize_adam(parameters)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "    \n",
    "        predictions = predict(X_test, parameters)\n",
    "        p = np.where(predictions == Y_test.flatten())\n",
    "        accuracy = len(p[0]) / Y_test.shape[0]\n",
    "        test_accuracy.append(accuracy)\n",
    "                   \n",
    "        for j, minibatch in enumerate(minibatches):\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            \n",
    "            # Forward propagation. Inputs: \"minibatch_X, parameters\". Outputs: \"A3, cache\".\n",
    "            A3, cache = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Cost function. Inputs: \"A3, minibatch_Y\". Outputs: \"cost\".\n",
    "            cost =  compute_cost(A3, minibatch_Y)\n",
    "\n",
    "            # Backpropagation. Inputs: \"parameters, cache, minibatch_X, minibatch_Y\". Outputs: \"grads\".\n",
    "            grads = backward_propagation(parameters, cache, minibatch_X, minibatch_Y)\n",
    "\n",
    "            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "            # 没有使用任何优化\n",
    "            if optimization == 'GD':\n",
    "                parameters =  update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "                \n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            #使用优化算法进行优化\n",
    "            elif optimization == 'AdaGrad':\n",
    "                parameters, grads_squared = update_parameters_with_adagrad(parameters, grads, grads_squared, learning_rate, epsilon)\n",
    "            elif optimization == 'Momentum': \n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta1, learning_rate)\n",
    "            elif optimization == 'Nesterov':\n",
    "                parameters, v = update_parameters_with_nesterov_momentum(parameters, grads, v, beta1, learning_rate)\n",
    "            elif optimization == 'RMSprop':\n",
    "                parameters, grads_squared = update_parameters_with_rmsprop(parameters, grads, grads_squared, beta2, learning_rate, epsilon)\n",
    "            elif optimization == 'Adam':\n",
    "                t = t + 1\n",
    "                parameters, v, u = update_parameters_with_adam(parameters, grads, v, u, t, beta1, beta2, learning_rate, epsilon)\n",
    "            if j == 0:\n",
    "                J_history.append(cost)\n",
    "            if i % 10 == 0 and j == 0:\n",
    "                print(\"Cost after iteration %i: %f   The accuracy of the model on test set is: %f\" % (i, cost,test_accuracy[-1]))\n",
    "            \n",
    "\n",
    "    predictions = predict(X_test, parameters)\n",
    "    p = np.where(predictions == Y_test.flatten())\n",
    "    accuracy = len(p[0]) / Y_test.shape[0]\n",
    "    test_accuracy.append(accuracy)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(J_history)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(\"optimization = \" + optimization)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(test_accuracy)\n",
    "    plt.ylabel('accuracy on test')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(\"optimization = \" + optimization)\n",
    "    plt.show()\n",
    "\n",
    "    return J_history, test_accuracy, parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用普通梯度下降法训练模型(每次训练大概要用10分钟左右，为了节省时间，如果在10次迭代后在测试集精度没有达到0.85以上就可以停止了，下同)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='GD')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 梯度下降优化算法\n",
    "\n",
    "下面回顾几种上课讲过的优化算法，注意它们之间的差异与联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 AdaGrad  \n",
    "\n",
    "当$J(\\theta)$收敛到最低值附近时，因为步长$\\alpha$大小固定的原因，$J(\\theta)$会在最低值附近徘徊，而不能到达最低值。因此，AdaGrad的想法是随着迭代次数的增加降低学习率$\\alpha$，学习率$\\alpha$衰减的方式是\n",
    "$$\n",
    "\\alpha^t = \\frac{\\alpha}{\\sqrt{t+1}}\n",
    "$$\n",
    "其中t表示第t次迭代。\n",
    "\n",
    "如果梯度数值小，$J(\\theta)$的移动步长小，$J(\\theta)$在坡度平缓的区域内下降速度会变慢。AdaGrad使用均方根来加快$J(\\theta)$在平缓区域的下降速度。均方根的表示为\n",
    "$$\n",
    "\\sigma^t = \\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}(g^i)2}\n",
    "$$\n",
    "其中$g^i$表示历史的梯度值。AdaGrad 的更新参数公式是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^{t+1} & := \\theta^t - \\frac{\\alpha^t}{\\sigma^t} g^t \\\\\n",
    "& := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2}} g^t\n",
    "\\end{aligned}\n",
    "$$\n",
    "在坡度平缓的区域，均方根的数值小，梯度除以一个数值小的数会变大，从而加大了$J(\\theta)$移动步长，也因此加快梯度下降速度。但是，AdaGrad的缺点是，随着迭代次数的增大，均方根会越来越大，梯度趋近于0，导致训练提前停止。为了防止分母为0，我们给分母加上一个小数值$\\epsilon =10^{-7}$。\n",
    "$$\n",
    "\\theta^{t+1} := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2} + \\epsilon} g^t\n",
    "$$\n",
    "\n",
    "我们可以看到分母里会计算所有历史梯度值的平方和，所以在实现的时候不用保存所有的历史梯度值，只需要保存一个纪录所有历史梯度平方和的值即可。每个参数的历史梯度和初始值为0。  \n",
    "**Hint** ：使用`np.zeros()`,`grads_squared`的shape与对应的参数相同   \n",
    "**任务7**：初始化AdaGrad的历史梯度平方和  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_grads_squared(parameters):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :return: grads_squared \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 #神经网络的层数\n",
    "    grads_squared = {}\n",
    "    \n",
    "    # Initialize grads_squared\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        grads_squared[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n",
    "        grads_squared[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return grads_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_test = initialize_parameters()\n",
    "grads_squared = initialize_grads_squared(parameters_test)\n",
    "print('shape of grads_squared dW1 ',grads_squared['dW1'].shape)\n",
    "print('grads_squared db3: ',grads_squared['db3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分母的求和项是从$t=0$时的梯度一直到当前的梯度，所以算历史梯度平方和的要把当前的梯度平方先加进去。  \n",
    "**任务8**：完成AdaGrad算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adagrad(parameters, grads, grads_squared, learning_rate=0.001, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :param grads: python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    :param grads_squared: python dictionary containing the current velocity:\n",
    "                    grads_squared['dW' + str(l)] = ...\n",
    "                    grads_squared['db' + str(l)] = ...\n",
    "    :param learning_rate: the learning rate, scalar\n",
    "    :param epsilon: epsilon, scalar\n",
    "    :return: parameters， grads_squared \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # 神经网络的层数\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #计算历史梯度平方和\n",
    "        grads_squared[\"dW\" + str(l + 1)] += grads[\"dW\" + str(l+1)] ** 2\n",
    "        grads_squared[\"db\" + str(l + 1)] += grads[\"db\" + str(l+1)] ** 2\n",
    "        parameters[\"W\" + str(l + 1)] -=  learning_rate * grads[\"dW\" + str(l+1)] / (np.sqrt(grads_squared[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] -=  learning_rate * grads[\"db\" + str(l+1)] / (np.sqrt(grads_squared[\"db\" + str(l+1)]) + epsilon)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, grads_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='AdaGrad')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RMSprop  \n",
    "\n",
    "从AdaGrad算法的公式可看出，所有时刻的梯度都对当前的参数更新有影响。如果早先的梯度并不是一个正确的方向，那么这些糟糕的梯度还是会影响到当前的参数更新。因此，RMSprop相当于就是只记录当前时刻前的某一段历史梯度和而不是所有历史梯度和。  \n",
    "RMSprop算法的公式如下：  \n",
    "$$\n",
    " u^0 = 0 \\\\\n",
    " u^{t+1} = \\rho u^t + (1-\\rho) [\\nabla J(\\theta ^t)]^2 \\\\ \n",
    " \\theta^{t+1} = \\theta^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}\\nabla J(\\theta ^t) \n",
    "$$\n",
    "这里$\\rho$是超参数，一般设为0.999，也不会调它。$\\epsilon$是防止分母为0。另外值得注意的是，因为要整合这几个算法在一起，而Adam算法又融合了各种算法，所以，关于优化算法的超参数的命名与Adam里保持一致，公式里的$\\rho$用下面参数`beta`代替。这些算法几乎都要保存一些变量，它们的初始化基本与AdaGrad初始化的方法一致，所以这部分初始化的代码就不重复了。  \n",
    "**任务9**：完成RMSprop算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_rmsprop(parameters, grads, grads_squared, beta=0.999, learning_rate=0.001, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :param grads: python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    :param grads_squared: python dictionary containing the current velocity:\n",
    "                    grads_squared['dW' + str(l)] = ...\n",
    "                    grads_squared['db' + str(l)] = ...\n",
    "    :param beta: the RMSprop hyperparameter, scalar\n",
    "    :param learning_rate: the learning rate, scalar\n",
    "    :param epsilon: epsilon, scalar\n",
    "    :return: parameters , grads_squared\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # 神经网络的层数\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        grads_squared[\"dW\" + str(l+1)] =  beta * grads_squared[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l + 1)] ** 2\n",
    "        grads_squared[\"db\" + str(l+1)] = beta * grads_squared[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l + 1)] ** 2\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)] / (np.sqrt(grads_squared[\"dW\" + str(l+1)]) + epsilon) \n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)] / (np.sqrt(grads_squared[\"db\" + str(l+1)]) + epsilon)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, grads_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='RMSprop')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Momentum\n",
    "\n",
    "动量梯度下降（Gradient Descent with Momentum）基本思想就是计算梯度的指数加权平均数，并利用该指数加权平均数更新权重。具体过程为：\n",
    "$$\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho v^t +\\alpha \\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "\n",
    "这里的$\\rho$一般取0.9。  \n",
    "**任务10**：完成Momentum算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta=0.9, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :param grads: python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    :param v: python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    :param beta: the momentum hyperparameter, scalar\n",
    "    :param learning_rate: the learning rate, scalar\n",
    "    :return: parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # 神经网络的层数\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #计算动量\n",
    "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] =  beta * v[\"db\" + str(l+1)] + learning_rate * grads[\"db\" + str(l+1)]\n",
    "        #更新参数\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - v[\"db\" + str(l+1)]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='Momentum')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Nesterov Momentum \n",
    "\n",
    "Nesterov Momentum算法与Momentum不同的是在于，它会提前计算一个在速度作用后的梯度。具体算法如下：\n",
    "$$\n",
    "v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t - \\rho v^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "但是在实现的时候，我们是不会算一次$J(\\theta ^t)$再算一次$\\nabla J(\\theta ^t - \\rho v^t)$的。具体编程实现时上式等价于下式：\n",
    "$$\n",
    " v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t) \\\\\n",
    " \\theta^{t+1} = \\theta ^t - \\rho v^{t+1} - \\alpha \\nabla J(\\theta ^t)\n",
    "$$\n",
    "这里的$\\rho$一般取0.9。   \n",
    "**任务11**:Nesterov Momentum算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nesterov momentum\n",
    "def update_parameters_with_nesterov_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using nesterov_momentum\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    v -- python dictionary containing your updated velocities\n",
    "\n",
    "    '''\n",
    "    VdW = beta * VdW + learning_rate * dW\n",
    "    Vdb = beta * Vdb + learning_rate * db\n",
    "    W = W - beta * VdW - learning_rate * dW\n",
    "    b = b - beta * Vdb - learning_rate * db\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers in the neural networks\n",
    "\n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l + 1)] -= (beta * v[\"dW\" + str(l + 1)]  + learning_rate * grads[\"dW\" + str(l + 1)])\n",
    "        parameters[\"b\" + str(l + 1)] -= (beta * v[\"db\" + str(l + 1)]  + learning_rate * grads[\"db\" + str(l + 1)])\n",
    "\n",
    "    return parameters, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='Nesterov')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Adam\n",
    "\n",
    "Adam优化算法（Adaptive Moment Estimation）是将Momentum和RMSprop结合在一起的算法，具体过程如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}v^{t+1}\n",
    "$$\n",
    "从上式可以看到，在最开始更新时，$u^{t},v^{t}$都是很小的。所以需要对早期的更新进行一个bias correction。完整公式如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "u^{t+1}_{corrected} = \\frac{u^{t+1}}{1-\\rho_2^t} \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "v^{t+1}_{corrected} = \\frac{v^{t+1}}{1-\\rho_1^t} \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}_{corrected}}+\\epsilon}v^{t+1}_{corrected}\n",
    "$$\n",
    "\n",
    "其中，一般设$\\rho_1=0.9,\\rho_2=0.999$.$\\epsilon$也是防止分母过小或等于0.  \n",
    "**任务12**：完成Adam算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Adam更新参数\n",
    "def update_parameters_with_adam(parameters, grads, v, u, t, beta1 = 0.9, \n",
    "                                beta2 = 0.999, learning_rate = 0.001, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    :param parameters: python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :param grads: python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    :param v: Adam variable, moving average of the first gradient, python dictionary\n",
    "    :param u: Adam variable, moving average of the squared gradient, python dictionary\n",
    "    :param t: epoch number\n",
    "    :param beta1: Exponential decay hyperparameter for the first moment estimates \n",
    "    :param beta2: Exponential decay hyperparameter for the second moment estimates \n",
    "    :param learning_rate: the learning rate, scalar.\n",
    "    :param epsilon: hyperparameter preventing division by zero in Adam updates\n",
    "    :return: parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    u_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"u, grads, beta2\". Output: \"u\".\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        u[\"dW\" + str(l+1)] =  beta2 * u[\"dW\" + str(l+1)] + (1 - beta2) * grads[\"dW\" + str(l+1)] ** 2\n",
    "        u[\"db\" + str(l+1)] =  beta2 * u[\"db\" + str(l+1)] + (1 - beta2) * grads[\"db\" + str(l+1)] ** 2\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"u, beta2, t\". Output: \"u_corrected\".\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        u_corrected[\"dW\" + str(l+1)] = u[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        u_corrected[\"db\" + str(l+1)] = u[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, u_corrected, epsilon\". Output: \"parameters\".\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] -=  learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(u_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] -=  learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(u_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters, v, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始训练模型并计算精度\n",
    "J_history, test_accuracy, parameters = nn_model(X_train, Y_train, num_iterations=100, optimization='Adam')\n",
    "print('The accuracy of the model on test set is:', test_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 总结\n",
    "本次实验完整搭建了一个三层的全连接网络，使用了各种梯度更新优化算法训练MNIST数据集。  \n",
    "提交作业后，可以试试提升精确度到97%以上？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
